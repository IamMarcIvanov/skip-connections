- The main folder is `tabular` and the main file is `tabular_main.py`
- You can change the config in `tabular/config.py` to the correct paths (for data and storing runs) and you can choose what model you want to run. The options are `SkipNN_1` which contains a skip connection and `NN_1` which does not contain a skip connection
- You will also have to correspondingly change the code for the skip connection neural network in `tabular/skip_nn.py`.
- The options in that file are to use the rotation line or the permutation line or neither rotation nor permutation. The rotation and permutation lines are commented out
- I did not use if else since that might have slowed things down. But perhaps there is automated optimization during the compilation so it could be put in
- Another option that you can change is the description in the config file - this will be put into the name of the run file
- The `runs` folder contains the results of the runs
- Multiple files are produced each run:
    - two are for the architecture (the one without extension and the pdf)
    - one is the png plot of loss and accuracy for training and testing
    - the txt file contains a few things:
        - a print of the model
        - the forward function
        - param counts
        - per epoch train and test loss and accuracy
    - the pth file contains the best found model (state dict)
- the results file contains some of the images that collate the results of multiple runs together - they were generated by changing the code in the `analyse_results.py` file
- the `resnet_func.py` file contains the code for the CNN with and without skip connections - for this I have not yet split it into a separate module so the same file contains a lot of the code.
- the `loss_visualisation.py` creates plots of linear interpolation loss between two weights, plots the norm of the weights as they change epoch to epoch, plots the distribution of weights for the final epoch - this corresponds to figures 2 and 3 in the paper "Visualizing the Loss Landscape of Neural Nets" by Li et. al.
